# 损失函数

![image-20250401203302491](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250401203302491.png)

可以理解为 上面 通过Loss 去提升 Loss越低越好

```
1.计算差距
2.更新输出提供依据 （ 反向传播）
```

#### [`nn.L1Loss`](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss)

```
|OUT-target| 求平均 
```

![image-20250401203705269](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250401203705269.png)

#### [`nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)

均方误差

![image-20250401203948752](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250401203948752.png)

```
|OUT-target|**2 求平均 
```

#### [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)

![image-20250401204156830](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250401204156830.png)

![image-20250401204211579](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250401204211579.png)

```
Loss(x,class) = Loss([0.1,0.2,0.3],1) = -0.2 + log(exp(0.1) + exp(0.2) + exp(0.3))
```

## 预测结果

```
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from torch.utils.tensorboard import SummaryWriter
from torch import nn

# 数据加载
train_data = datasets.CIFAR10("./data", train=True, transform=transforms.ToTensor())
test_data = datasets.CIFAR10("./data", train=False, transform=transforms.ToTensor())

class FixCIFAR10(nn.Module):
    # def __init__(self):
    #     super(FixCIFAR10, self).__init__()
    #     self.conv2d = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=5,stride= 1,padding=2)
    #     self.maxpool1 = nn.MaxPool2d(2)
    #     self.conv2d2 = nn.Conv2d(in_channels=32,out_channels=32,kernel_size=5,padding=2)
    #     self.maxpool2 = nn.MaxPool2d(2)
    #     self.conv2d3 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=5,padding=2)
    #     self.maxpool3 = nn.MaxPool2d(2)
    #     self.Flatten1 = nn.Flatten()
    #     self.liner1 = nn.Linear(1024,64)
    #     self.liner2 = nn.Linear(64,10)
    #
    # def forward(self,x):
    #     x = self.conv2d(x)
    #     x = self.maxpool1(x)
    #     x = self.conv2d2(x)
    #     x = self.maxpool2(x)
    #     x = self.conv2d3(x)
    #     x = self.conv2d3(x)
    #     x = self.Flatten1(x)
    #     x = self.liner1(x)
    #     return self.liner2(x)

    def __init__(self):
        super(FixCIFAR10, self).__init__()
        self.modul1 = nn.Sequential(  #使用Sequentiall简化
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),
            nn.MaxPool2d(2),
            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2),
            nn.MaxPool2d(2),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(1024, 64),
            nn.Linear(64, 10)
        )

    def forward(self,x):
        return self.modul1(x)

ez = FixCIFAR10()

train_loader = DataLoader(train_data,batch_size=1)
Writer = SummaryWriter("./logs")
for epoch in range(2):
    step = 1
    for data in train_loader:
        imgs , targets = data
        images = ez(imgs)
        print(images)
        print(targets)
        step +=1

Writer.close()
```

![image-20250401205548853](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250401205548853.png)

 下面是 指定类别 上面是各个概率分布 

后面的grad_fn 是梯度 可以用于反向传播