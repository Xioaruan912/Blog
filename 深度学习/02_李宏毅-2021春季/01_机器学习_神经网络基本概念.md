# Machine Learning 和 Deep Learning

## Machine Learning 机器学习

机器学习 本质上就是 Looking for Function 查找函数的能力

假如我们需要机器 去识别声音认知 那么可以通过下面的图片概括

![image-20250321142214793](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321142214793.png)

这个函数 人类难以获取 通过机器的测试 就可以找到 这样就是 机器学习

![image-20250321142637789](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321142637789.png)

随着机器学习不同就出现不同的类别

**Regression** ： 输出一个数值 例如预测未来某个时间点的 PM2.5数值 这就是regression

**Classification**： 选择一个结果 例如 查看GMAIL 里面是不是垃圾邮件 选择Yes or No

但是 机器学习不只是 这两个 例如 **Structured Learing** 结构化学习 这个是很难的

### 机器如何找函数

例如 李宏毅老师的 YOUTUBE 后台点击量 / 播放量 预测 未来的 播放量

![image-20250321143618057](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321143618057.png)

### 三步骤

#### 1.写出数学函数

$$
y = b + wx
$$

y ： 预测的内容

x ： 前一天观看人数

b  w :    需要找出来的 参数 bias 和 weight

这个只是一个猜测的函数式

```
我们可以理解为 y = 前一天的观看 * w  存在某种关系 但是不是这个简单的关系
所以我们 要加一个 b 进行修正
```

所以 这个带有未知参数的函数 就是 模型  **model**

#### 2.定义 Loss

Loss 也是一个函数  是一个输入 b和w 的函数
$$
L(b,w)
$$
用于检验 当下 b 和 w 的情况下 结果多好 

![image-20250321144238100](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321144238100.png)

首先我们需要一个训练集

![image-20250321144311286](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321144311286.png)

然后将 函数表达式的x 用训练的数据带入 计算出的y 和 训练数据真实结果 我们叫做 **label** 比较 就可以得到这个b w 是否精准 

![image-20250321144523904](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321144523904.png)

我们可以将所有的 e 算出来  

将所有的 e 相加取平均 就可以得到最终的Loss

![image-20250321144743157](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321144743157.png)

如果 Loss 越大 代表 bw 越差 反之越好

这里损失函数不局限于这个表达式

![image-20250321144852968](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321144852968.png)

![image-20250321145008569](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321145008569.png)

通过穷举b和w 产生的 就是 这个图片 我们叫做 **Error SurFace** 

#### 3.Optimization

假设 一个参数 引出计算方法

我们要找到最好的 wb 使用 **Gradient Descent** **梯度下降**

我们假设只有一个参数 w w*为最好参数

![image-20250321145313315](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321145313315.png)

我们就会重新得到一个 Error Surface 

![image-20250321145338083](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321145338083.png)

关于 W和L的  随机选取初始的 w

接下来通过  计算导数 获取 L 在 w = w0的 时候的微分

![image-20250321145508643](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321145508643.png)

如果找到  导数 > 0  那么增大 w 否则 减小 w

w增大/减小的长度根据 **斜率 和 一个学习速率** 是自己决定的 我们叫做 **hyperparameter**

![image-20250321145726072](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321145726072.png)

然后反复进行计算 直到 导数为0

![image-20250321151023112](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321151023112.png)

这里会存在巨大问题 没有找到最小的地点 本质上后面那个全局最小 没有找到 而是找到了 局部最优

![image-20250321151152159](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321151152159.png)

两个参数

![image-20250321151411471](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321151411471.png)

计算对 L的微分 = 0

![image-20250321151439278](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321151439278.png)

然后在通过 hyperparamters 计算

![image-20250321151504675](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321151504675.png)

我们可以通过PyTorch 就可以计算出微分的值

我们通过两个参数如下

![image-20250321151737504](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321151737504.png)

这样就可以获取到最优解

我们这样计算出 训练的误差 这样我们可以通过这个算法 预测未来的数据 计算 L‘ 

### 使用模型预测

![image-20250321152142729](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321152142729.png)

红色是真实的数据 蓝色是预测 

这里我们可以发现存在周期性 如果我们的x 使用7天数据呢

![image-20250321152400635](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321152400635.png)

7天的观看人数  * w 想加 修正 获取 到预测未来的 可以得到更好的Loss

![image-20250321152453447](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321152453447.png)

看看考虑不同天数

![image-20250321152634481](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321152634481.png)

我们将这种模型 叫做 linear model 线形模型

![image-20250321152724212](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321152724212.png)

使用线形的话 就是 一直都是一个直线  但是现实并不一定是这样的 所以 线形模型 并不是最好的 存在很大的限制 叫做 **Model Bias** 这样我们需要写一个更有弹性的model

## Deep Learning 深度学习

###  更弹性的model

#### 创建new _function / model

![image-20250321152945137](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321152945137.png)

constant 就是 和y交点 蓝色的funciton 也要注意 

![image-20250321153029203](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321153029203.png)

使用 函数 0 + 函数 1 就可以得到 红色 function 第一个线段 到最高点前面那个

我们第二个转折点开始和结束地点 画出function 2

![image-20250321153140133](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321153140133.png)

如果用012 相加 那么就可以获取到红色function第二个转折点 同理增加 第四个function

![image-20250321153244198](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321153244198.png)

蓝色相加后 就可以得到 红色function

这些 **Piecewise Linear Curves** 就是红色转折点  可以通过constant 和 蓝色函数组合而成

![image-20250321153405730](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321153405730.png)

用直线代替曲线 微分学 有点泰勒公式的感觉

![image-20250321153458393](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321153458393.png)

但是我们可以通过一个曲线逼近蓝色funciton

![image-20250321153607635](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321153607635.png)

这个函数 我们叫做 **sigmoid function** 

![image-20250321153722876](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321153722876.png)

那我们需要通过 sigmoid function 逼近 hard sigmoid function 然后通过 线性组合 + 常数 再次逼近 红色real linear

那么现在问题就是 我们如何构造出不同的 sigmoid funciton 

改变不同值对 sigmoid function的作用 这里只要改变w b c 就可以构造出不同的 function

![image-20250321154000521](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321154000521.png)

通过这个 肯定可以构造出不同的 function

所以结果为 这里c  w b 都各不相同

![image-20250321154049256](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321154049256.png)

结果为![image-20250321154124497](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321154124497.png)

这就是更弹性的 红色 model

所以我们的model 变为如下

![image-20250321154248756](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321154248756.png)

这里 就是我们红色 function 构造的过程

b为 偏离值 然后后面是多个 sigmoid函数 组合在一起

那么我们的 多天模型 就可以转变为如下

![image-20250321154841991](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321154841991.png)

i ： 几个sigmoid function

b： 修正值

j ： 天数

在 sigmoid function里面 的 算法 是计算出不同的 sigmoid funciton 传入值

![image-20250321155154948](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321155154948.png)

这样 我们就有 3个sigmoid function 相加  这样 我们是不是就可以 通过一个constant 和 蓝function 得到红色 函数 model

![image-20250321155339884](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321155339884.png)

R1  R2 R3 就是通过sigmoid函数的

我们可以转变为 线性代数 表示方式

![image-20250321155436774](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321155436774.png)

最后 

![image-20250321155555973](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321155555973.png)

最终得到了y  ![image-20250321155712719](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321155712719.png)

x ： feature

其余都是 unknown parameters 将未知参数 拼起来如下

![image-20250321155840800](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321155840800.png)

#### Loss

变为了 ![image-20250321160411312](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321160411312.png)

计算方法一样的 设定训练集 然后给一个 x 计算误差 误差计算后的得到 Loss

#### Optimization

还是一样 获取最佳 unknown parameters 

![image-20250321160535616](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321160535616.png)

上面我们记为

![image-20250321160640122](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321160640122.png)

然后进行更新参数

![image-20250321160724193](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321160724193.png)

所以获取最好 参数步骤如下

![image-20250321160837026](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321160837026.png)

这样我们减少 训练集 

![image-20250321160946402](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321160946402.png)

实际上如下

![image-20250321161009822](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321161009822.png)

如果我们通过 batch 这种方法训练 那么 1次 epoch 就会产生1000次的 update

如果我们通过 分段函数 ReLU方法 就会变成如下

#### ReLU

![image-20250321183943207](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321183943207.png)

这里就是relu的基本思想 通过 max 但是这样我们需要通过超过两倍的relu函数式子

![image-20250321161611171](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321161611171.png)

继续修改模型

![image-20250321184141481](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321184141481.png)

我们之前通过relu或者 sigmoid 计算获取到 a1 a2 a3 那么我们再次多次到 做几次 

![image-20250321184233713](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321184233713.png)

最后可以得到 a` 

这里出现的模型和想法是什么 

这里的 sigmoid 或者 relu 我们叫做 neuron 神经元

这一串 model 就是 neuron network

我们后面改为neuron 改为  hidden layer多个

我们就叫做 deep learning

但是如果我们 测试上很好 但是在 预测中准确率极低

![image-20250321184642473](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250321184642473.png)

我们叫做过**拟合** **overfitting**