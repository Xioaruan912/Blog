我们之前学习了 gredient decent **梯度下降法**

我们知道 梯度下降法 没办法给我们一个最好loss的function 他只会给第一个 导数为0的

# Overfitting 过拟合

给出一个什么都没有用的model

![image-20250331145141749](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331145141749.png)

我们可以知道 在Train_data上 loss为0 但是在Test_data上Loss会很大 因为什么都没有学习到

我们首先看看如果给定资料少了

![image-20250331163815212](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331163815212.png)

蓝色代表 训练资料 橙色代表测试资料 这是虚线代表这是我们真正意义上的规律 只是我们现在无法发现

那么我们函数有可能训练为

![image-20250331164000727](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331164000727.png)

可以发现结果如下

![image-20250331165858502](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331165858502.png)

所以可以发现 比较有弹性的model 他就会容易出现overfitting

## 解决OVERfitting

### 1.增加训练资料 

如果蓝色变多了  虽然弹性变大 但是还是有可能能够拟合

![image-20250331170018736](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331170018736.png)

但是这里就是需要花时间收集资料

### 2.创造资料 

例如 左右翻转 镜像翻转

![image-20250331170111968](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331170111968.png)

需要是一个合理的东西

### 3.限制Model

例如强制限制model为二次曲线

![image-20250331170221856](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331170221856.png)

那么就有可能拟合

![image-20250331170306862](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331170306862.png)

可以拟合到 最终的规则

少参数 少神经元 或者公用参数 那么CNN就是有限制的 比较没有弹性的 影像特效的model

### 4.早停止 early stopping

但是我们也不能给太多的限制

![image-20250331170546544](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331170546544.png)

如下是一个机器学习的 test和train下loss的状态 随着函数负责变化

![image-20250331170732586](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331170732586.png)

可以发现当函数越复杂 在训练资料中显示更好 但是测试资料确实越来越糟糕 所以我们给定的函数 需要依照特点限制 才可以能够 更好的在测试资料上展示出更好的Loss

但是我们如果要好好选择模型 应该如下

![image-20250331172222220](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331172222220.png)

将训练资料分为不同的 然后测试 获取最好

最好可以对训练资料N等分 直接获取到 这样就可以出现一个最好的 model

![image-20250331172637165](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331172637165.png)

### 如何判定是什么导致模型无法训练

![image-20250331173543136](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331173543136.png)

我们之前学习了 local minima 但是并不是这样的

![image-20250331173616380](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331173616380.png)

我们可以发现 在马鞍的图像中 既不是最小也不是最大 但是他的导数就是0 那么这种点 我们叫做 crtical point **临界点**

如果我们卡在 local minima 那么真是无路可走 但是如果是saddle point 那么还是可以训练的

我们可以通过泰勒展开 实现 查找Critcial point 点 

```
例如 如果x 为零界点 那么 f(x) = f(x')+1/2*f'(x') + 1/4 * f''(x')
```

这个时候我们的Loss最低 导致 一阶导数为0。那么 fx 就取消了中间的一阶导 转变为 f(x') + 1/4*f''(x')

那么我们其实可以通过计算 二阶导 最终得到是否处于 local minima

![image-20250331174555101](https://raw.githubusercontent.com/Xioaruan912/pic/main/image-20250331174555101.png)

这里其实可以通过 二阶导数的正负 判断凹凸

```
 二阶导数正 凹函数 那么代表是 就是局部最小点 其他都比它大 就是 local minima
 二阶导数负 凸函数 那么代表是 局部最大点  local maxima
 二阶导数正负交替 那么就是 saddle point 
```

